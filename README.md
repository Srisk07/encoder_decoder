# Leveraging Encoder-Decoder Architecture for Effective Poem Summarization

##  Overview

This repository contains a research paper titled **"Leveraging Encoder-Decoder Architecture for Effective Poem Summarization"** authored by Danusya S, Roshini T, Sri Harini S, Mutharasi K, and Arunkumar N from Bharathiar University.

The study focuses on the use of the **T5 (Text-to-Text Transfer Transformer)** model to generate meaningful summaries of poems. Poetry presents unique challenges in NLP due to abstract themes, figurative language, and emotional depthâ€”this paper proposes a solution using advanced encoder-decoder architecture.

##  Key Topics

- Natural Language Processing (NLP)
- Text Summarization
- Encoder-Decoder Architecture
- T5 Transformer Model
- Poem Preprocessing and Tokenization
- Attention Mechanism
- Beam Search
- Pseudo-code for Implementation

##  Methodology

1. **Dataset**: Poetry Foundation Dataset.
2. **Preprocessing**: Tokenization, lowercasing, punctuation removal.
3. **Model**: T5 (Encoder-Decoder architecture with self- and cross-attention).
4. **Summary Generation**: Generated using beam search decoding.
5. **Output**: Summarized version of poetic texts maintaining emotional and thematic essence.

##  Results & Conclusion

- T5 model performed well on summarizing poems.
- Generated summaries preserved the core meaning and sentiment.
- Demonstrates potential for literary NLP applications.

##  Future Enhancements

- Multilingual poem summarization
- Integration with Text-to-Speech (TTS)
- Personalized summarization
- Educational tool development

## ðŸ“„ File

- [`encoder_decoder.pdf`](encoder_decoder.pdf) â€“ Full research paper with methodology, pseudo-code, and results.

##  Authors

- Danusya S  
- Roshini T  
- Sri Harini S  
- Mutharasi K  
- Arunkumar N

## Contact

For queries or collaborations:
- Sri Harini S â€“ [harinikumar215@gmail.com](mailto:harinikumar215@gmail.com)

---

